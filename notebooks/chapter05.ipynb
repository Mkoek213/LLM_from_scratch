{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e228a6",
   "metadata": {},
   "source": [
    "## Pretraining on unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f13d70",
   "metadata": {},
   "source": [
    "### 1) Evaluating generative text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47cd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GPTModel\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 200019,\n",
    "    \"context_length\": 16,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba679878",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08ab0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from utils import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cfb5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[152687,   8811,   3705, 155653,    286,   2453,   3008,  12637]])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Pierwszy dzień wiosny jest\"\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "input_ids = text_to_token_ids(start_context, tokenizer)\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06c7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    decoded = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73251bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs to text: <function token_ids_to_text at 0x750c3c7a8f40>\n"
     ]
    }
   ],
   "source": [
    "token_ids_to_text(input_ids, tokenizer)\n",
    "print(\"Token IDs to text:\", token_ids_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2126a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4116fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([152687,   8811,   3705, 155653,    286,   2453,   3008,  12637, 114233,\n",
       "        131507,  11345,  36742,  83014,  89495, 111590,  20821,  61375,   1952])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b33575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierwszy dzień wiosny jestpokemon jasmineissetď insign обмен രാവിലെ_htmlическая even'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9b2e5",
   "metadata": {},
   "source": [
    "### 2) Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf67719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    54, 148556,  51201,   6517,   6248, 104788,  21589,    621]])\n",
      "tensor([[100256,   6248, 104788,  21589,    621,    460,  28178,     84]])\n"
     ]
    }
   ],
   "source": [
    "inputs = text_to_token_ids(\"Wszystkie drogi prowadzą do\", tokenizer)\n",
    "print(inputs)\n",
    "targets = text_to_token_ids(\"drogi prowadzą do Rzymu\", tokenizer)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0399ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f35bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 200019])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e940914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 200019])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394f1e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.8463e-06, 3.3196e-06, 2.2871e-06,  ..., 4.6453e-06,\n",
       "          4.4006e-06, 5.5665e-06],\n",
       "         [1.2039e-05, 8.7879e-06, 9.5373e-06,  ..., 5.9818e-06,\n",
       "          6.3712e-06, 5.5118e-06],\n",
       "         [3.2204e-06, 7.9758e-06, 7.4833e-06,  ..., 1.2257e-05,\n",
       "          1.9118e-06, 4.0632e-06],\n",
       "         ...,\n",
       "         [8.8964e-06, 2.9440e-06, 2.8775e-06,  ..., 4.4353e-06,\n",
       "          3.6763e-06, 8.4586e-06],\n",
       "         [2.2601e-06, 4.0622e-06, 5.5779e-06,  ..., 5.9962e-06,\n",
       "          1.3513e-05, 6.6666e-06],\n",
       "         [4.6421e-06, 5.6666e-06, 8.6539e-06,  ..., 1.3515e-06,\n",
       "          2.3597e-06, 2.9618e-06]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aa47c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[[ 18066],\n",
      "         [199946],\n",
      "         [155917],\n",
      "         [170164],\n",
      "         [122979],\n",
      "         [ 34270],\n",
      "         [188749],\n",
      "         [145830]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8206f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch: drogi prowadzą do Rzymu\n",
      "Outputs batch: 『ucklesangaza الفندق भारी trap ยัง reakc\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f32346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target probabilities: tensor([2.8739e-06, 5.8466e-06, 2.2059e-06, 5.4642e-06, 5.1265e-06, 3.7524e-06,\n",
      "        3.6182e-06, 2.5854e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas = probas[text_idx, -1, targets[text_idx]]\n",
    "print(\"Target probabilities:\", target_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b585bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities: tensor([-12.7598, -12.0497, -13.0244, -12.1173, -12.1811, -12.4931, -12.5295,\n",
      "        -12.8656])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(target_probas)\n",
    "print(\"Log probabilities:\", log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b5ad618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.5026)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 * torch.mean(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31b55d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.6252)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1),\n",
    "    targets.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593cc43",
   "metadata": {},
   "source": [
    "### 3) Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9acdd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataloader_v1\n",
    "\n",
    "text_file = \"../data_brzechwa.txt\"\n",
    "text_data = open(text_file, \"r\", encoding=\"utf-8\").read()\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(text_data) * train_ratio)\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65960c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=1,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=1,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b77bb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: \n",
      "torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader: \")\n",
    "for x, y in train_loader:\n",
    "    pass\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa1f7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loader: \n",
      "torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "print(\"Val loader: \")\n",
    "for x, y in val_loader:\n",
    "    pass\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f899cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 183088\n",
      "Val tokens: 20160\n",
      "Total tokens: 203248\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(f\"Train tokens: {train_tokens}\")\n",
    "print(f\"Val tokens: {val_tokens}\")\n",
    "print(f\"Total tokens: {train_tokens + val_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e08bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a9b9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c347240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 12.3800\n",
      "Val loss: 12.3900\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\"Train loss: {train_loss:.4f}\")\n",
    "print(f\"Val loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a8b56b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([238852.2031])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor([12.3836]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891513e",
   "metadata": {},
   "source": [
    "### 4) Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e799449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idna import decode\n",
    "from sympy import evaluate\n",
    "import torch\n",
    "from utils import GPTModel\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1}, Step {global_step}, \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Tokens Seen: {tokens_seen}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5d41728",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94bc57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16642de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59349/2966103473.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_59349/2966103473.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/mikolaj/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 239.62 MiB is free. Including non-PyTorch memory, this process has 3.18 GiB memory in use. Of the allocated memory 2.94 GiB is allocated by PyTorch, and 184.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_loses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPierwszy dzień wiosny jest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     17\u001b[39m     loss = calc_loss_batch(input_batch, target_batch, model, device)\n\u001b[32m     18\u001b[39m scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m scaler.update()\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:356\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    354\u001b[39m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     retval = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/optim/adam.py:236\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    233\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     has_complex = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m     adam(\n\u001b[32m    247\u001b[39m         params_with_grad,\n\u001b[32m    248\u001b[39m         grads,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         decoupled_weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/LLM_from_scratch/venv/lib/python3.11/site-packages/torch/optim/adam.py:176\u001b[39m, in \u001b[36mAdam._init_group\u001b[39m\u001b[34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[39m\n\u001b[32m    166\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    167\u001b[39m     torch.zeros(\n\u001b[32m    168\u001b[39m         (),\n\u001b[32m   (...)\u001b[39m\u001b[32m    173\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m torch.tensor(\u001b[32m0.0\u001b[39m, dtype=_get_scalar_dtype())\n\u001b[32m    174\u001b[39m )\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[32m    180\u001b[39m state[\u001b[33m\"\u001b[39m\u001b[33mexp_avg_sq\u001b[39m\u001b[33m\"\u001b[39m] = torch.zeros_like(\n\u001b[32m    181\u001b[39m     p, memory_format=torch.preserve_format\n\u001b[32m    182\u001b[39m )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 239.62 MiB is free. Including non-PyTorch memory, this process has 3.18 GiB memory in use. Of the allocated memory 2.94 GiB is allocated by PyTorch, and 184.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_loses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Pierwszy dzień wiosny jest\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de522d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_seen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m     plt.show()\n\u001b[32m     23\u001b[39m epochs_seen = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m plot_losses(epochs_seen, \u001b[43mtokens_seen\u001b[49m, train_loses, val_losses)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokens_seen' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label='Train Loss', color='blue')\n",
    "    ax1.plot(epochs_seen, val_losses, label='Val Loss', color='orange', linestyle='--')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel('Tokens Seen')\n",
    "\n",
    "    fig.tight_layout\n",
    "    plt.savefig(\"losses_plot.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_seen = list(range(1, num_epochs + 1))\n",
    "plot_losses(epochs_seen, tokens_seen, train_loses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063578a6",
   "metadata": {},
   "source": [
    "### 5) Decoding strategies to control randomness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "967bd56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Pierwszy dzień wiosny jestpokemon jasmineissetď insign обмен രാവിലെ_htmlическая even kys Graduate24драв21 Constructs المعلومات Javaтич очень')\n",
      "\n",
      " permane$thisushortaptureaneers thymestücke furt/raw(Object无码亚洲 天天爱彩票是Thicknessვალ\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Pierwszy dzień wiosny jest\", tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(f\"Output text: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c458b0c",
   "metadata": {},
   "source": [
    "### 5.1) Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ababea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import inverse\n",
    "\n",
    "\n",
    "vocab = {\n",
    "    \"blisko\": 0,\n",
    "    \"drogi\": 1,\n",
    "    \"jest\": 2,\n",
    "    \"pierwszy\": 3,\n",
    "    \"wiosny\": 4,\n",
    "    \"do\": 5,\n",
    "    \"rzymu\": 6,\n",
    "    \"wszystkie\": 7,\n",
    "    \"prowadzą\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22974f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5915, 0.0052, 0.0199, 0.0102, 0.0033, 0.0675, 0.0014, 0.2851, 0.0158])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, -0.23, 1.12, 0.45, -0.67, 2.34, -1.56, 3.78, 0.89]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0670991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token ID: 0\n"
     ]
    }
   ],
   "source": [
    "next_token_id = torch.argmax(probas).item()\n",
    "print(\"Next token ID:\", next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e8e1424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wszystkie\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a980d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 x blisko\n",
      "8 x drogi\n",
      "20 x jest\n",
      "12 x pierwszy\n",
      "7 x wiosny\n",
      "66 x do\n",
      "1 x rzymu\n",
      "279 x wszystkie\n",
      "24 x prowadzą\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(1000)]\n",
    "    sample_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sample_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b3a0aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.5915, 0.0052, 0.0199, 0.0102, 0.0033, 0.0675, 0.0014, 0.2851, 0.0158]),\n",
       " tensor([9.9932e-01, 2.5951e-21, 1.8929e-15, 2.3300e-18, 3.1860e-23, 3.7628e-10,\n",
       "         4.3454e-27, 6.7508e-04, 1.8978e-16]),\n",
       " tensor([0.2005, 0.0777, 0.1018, 0.0890, 0.0711, 0.1299, 0.0595, 0.1733, 0.0972])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import softmax\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, temp) for temp in temperatures]\n",
    "\n",
    "scaled_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37a8060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i, temp in enumerate(temperatures):\n",
    "    rects = ax.bar(\n",
    "        x + i * bar_width, \n",
    "        scaled_probas[i], \n",
    "        width=bar_width, \n",
    "        label=f'Temperature {temp}'\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature_scaling.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5fd6a",
   "metadata": {},
   "source": [
    "### 5.2) Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b04d160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, -0.23, 1.12, 0.45, -0.67, 2.34, -1.56, 3.78, 0.89]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fca2383a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.5100, 3.7800, 2.3400]), tensor([0, 7, 5]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, k=top_k)\n",
    "top_logits, top_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "550a7436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5100,   -inf,   -inf,   -inf,   -inf, 2.3400,   -inf, 3.7800,   -inf])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_logits = torch.where(condition=next_token_logits < top_logits[-1],\n",
    "                         input=torch.tensor(float(\"-inf\")),\n",
    "                         other=next_token_logits)\n",
    "new_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434b3c4",
   "metadata": {},
   "source": [
    "### 5.3) Modyfing the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c388cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, top_pos = torch.topk(logits, k=top_k)\n",
    "            logits = torch.where(condition=logits < top_logits[:, -1],\n",
    "                         input=torch.tensor(float(\"-inf\")),\n",
    "                         other=logits)\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e60ca1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Pierwszy dzień wiosny jest професSandbox 尚seiယူztat segn второго ma populated\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    temperature=1.4,\n",
    "    top_k=25,\n",
    ")\n",
    "\n",
    "print(f\"Output text: {token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
